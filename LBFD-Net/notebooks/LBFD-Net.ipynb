{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTVvb1Gci9W8"
   },
   "source": [
    "**Let's check the GPUs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1742153868399,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "K2z8oSHpi8L8",
    "outputId": "352ab25e-dfc0-4c4a-efe3-cae744d08ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 16 19:37:47 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   54C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWUiDbxnevIy"
   },
   "source": [
    "**IMPORT NECESSARY LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1742153984092,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "27m522OpW0Bz"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Union\n",
    "\n",
    "# Scientific computing and numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch core modules\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "# PyTorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Computer vision and image processing\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do-vdVOTezN8"
   },
   "source": [
    "**Define the constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742153883453,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "ZPfC7oT4Y8lM"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = Path('/content/drive/MyDrive/fall_no_fall_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742153887143,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "eAZO9nJIW8Wm"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "CLASS_LABELS: Dict[str, int] = {\"no_fall\": 0, \"fall\": 1}\n",
    "\n",
    "# Normalization for 224x224 RGB images\n",
    "TRANSFORMS_DATASET_NORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1742153888053,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "1x-39v2tfTck"
   },
   "outputs": [],
   "source": [
    "SEED = 42  # Set a fixed seed for reproducibility\n",
    "\n",
    "random.seed(42)  # Set seed for Python's built-in random module\n",
    "np.random.seed(42)  # Set seed for NumPy random operations\n",
    "torch.manual_seed(42)  # Set seed for PyTorch CPU operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)  # Set seed for all CUDA devices\n",
    "\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n",
    "torch.backends.cudnn.benchmark = False  # Disable cuDNN benchmarking for consistency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u4-NlzTe4hS"
   },
   "source": [
    "**Define the dataset class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742153889893,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "B3e5t6NfXxVd"
   },
   "outputs": [],
   "source": [
    "class BinaryFallDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root_directory_path: Path,\n",
    "                 subset: str,\n",
    "                 image_transforms: transforms.Compose = TRANSFORMS_DATASET_NORM) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset with parameters only (no data loading).\n",
    "\n",
    "        Args:\n",
    "            root_directory_path (str | Path): Path to dataset directory containing 'train', 'validation', and 'test' subfolders.\n",
    "            subset (str, optional): Which subset to use: 'train', 'validation', or 'test'. Defaults to 'train'.\n",
    "            image_transforms (transforms.Compose, optional): Transformations for images. Defaults to [-1, 1] normalization.\n",
    "        \"\"\"\n",
    "        self.root_directory_path = Path(root_directory_path)\n",
    "        self.subset = subset  # \"train\", \"validation\", or \"test\"\n",
    "        self.image_transforms = TRANSFORMS_DATASET_NORM\n",
    "        self.image_file_paths: List[Path] = []\n",
    "        self.class_labels: List[int] = []\n",
    "        self.is_data_loaded = False\n",
    "\n",
    "    def load_dataset(self) -> None:\n",
    "        \"\"\"Load image file paths and labels from the dataset directory for the selected subset.\"\"\"\n",
    "        if self.is_data_loaded:\n",
    "            return  # Avoid reloading\n",
    "\n",
    "        subset_directory = self.root_directory_path / self.subset\n",
    "        if not subset_directory.exists():\n",
    "            raise FileNotFoundError(f\"Subset directory '{subset_directory}' not found.\")\n",
    "\n",
    "        # For each class (fall, no_fall), collect all PNG files\n",
    "        for class_name, class_label in CLASS_LABELS.items():\n",
    "            class_directory = subset_directory / class_name\n",
    "            if not class_directory.exists():\n",
    "                raise FileNotFoundError(f\"Directory '{class_directory}' not found.\")\n",
    "\n",
    "            image_files_in_class = list(class_directory.rglob(\"*.png\"))\n",
    "            self.image_file_paths.extend(image_files_in_class)\n",
    "            self.class_labels.extend([class_label] * len(image_files_in_class))\n",
    "\n",
    "        if not self.image_file_paths:\n",
    "            raise ValueError(\"No images found in the dataset directory!\")\n",
    "\n",
    "        self.is_data_loaded = True\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if not self.is_data_loaded:\n",
    "            raise RuntimeError(\"Dataset not loaded. Call `load_dataset()` first.\")\n",
    "        return len(self.image_file_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Load and transform a single image on demand.\"\"\"\n",
    "        if not self.is_data_loaded:\n",
    "            raise RuntimeError(\"Dataset not loaded. Call `load_dataset()` first.\")\n",
    "\n",
    "        image_file_path = self.image_file_paths[index]\n",
    "        class_label = self.class_labels[index]\n",
    "\n",
    "        image_data = Image.open(image_file_path).convert(\"RGB\")\n",
    "        image_tensor = self.image_transforms(image_data) if self.image_transforms else image_data\n",
    "\n",
    "        return image_tensor, class_label\n",
    "\n",
    "    def get_data_loader(self, batch_size: int = 16, shuffle_data: bool = True) -> DataLoader:\n",
    "        \"\"\"Return a DataLoader with minimal RAM usage.\"\"\"\n",
    "        if not self.is_data_loaded:\n",
    "            self.load_dataset()\n",
    "\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle_data, num_workers=2, pin_memory=True, prefetch_factor=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhCnhBS3e783"
   },
   "source": [
    "**Define the metric calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742153897108,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "sfiZwl1i0jlT"
   },
   "outputs": [],
   "source": [
    "def numpy_sigmoid(logits):\n",
    "    return 1 / (1 + np.exp(-logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1742153900375,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "5AOKZS_WXT_m"
   },
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Computes and stores binary classification metrics for fall detection using scikit-learn.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize lists to store predictions and ground truth labels.\"\"\"\n",
    "        self.ground_truth: List[int] = []\n",
    "        self.model_prediction: List[int] = []\n",
    "\n",
    "    def convert_logits(self, logits: Union[np.ndarray, torch.Tensor], targets: Union[np.ndarray, torch.Tensor], threshold: float = 0.5) -> None:\n",
    "        \"\"\"\n",
    "        Converts raw logits to binary predictions using a sigmoid threshold\n",
    "        and updates the stored ground truth and prediction lists.\n",
    "\n",
    "        Args:\n",
    "            logits (np.ndarray or torch.Tensor): Raw model outputs with shape [batch_size, 1].\n",
    "            targets (np.ndarray or torch.Tensor): Ground truth labels with shape [batch_size, 1] or [batch_size].\n",
    "            threshold (float): Threshold for converting probabilities to binary predictions (default: 0.5).\n",
    "        \"\"\"\n",
    "        # Convert PyTorch tensors to NumPy arrays if necessary\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if isinstance(targets, torch.Tensor):\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "        # Validate input shapes\n",
    "        if logits.ndim != 2 or logits.shape[1] != 1:\n",
    "            raise ValueError(f\"Logits must have shape [batch_size, 1], got {logits.shape}\")\n",
    "        if targets.shape[0] != logits.shape[0]:\n",
    "            raise ValueError(f\"Targets batch size ({targets.shape[0]}) must match logits ({logits.shape[0]})\")\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probabilities = numpy_sigmoid(logits)\n",
    "\n",
    "        # Threshold probabilities to obtain binary predictions\n",
    "        predicted_labels = (probabilities >= threshold).astype(int).flatten()\n",
    "\n",
    "        # Flatten targets (if not already 1D) and update lists\n",
    "        targets_list = targets.flatten().tolist()\n",
    "        predicted_labels_list = predicted_labels.tolist()\n",
    "\n",
    "        self.ground_truth.extend(targets_list)\n",
    "        self.model_prediction.extend(predicted_labels_list)\n",
    "\n",
    "    def precision(self) -> float:\n",
    "        \"\"\"Compute Precision = TP / (TP + FP).\"\"\"\n",
    "        return precision_score(self.ground_truth, self.model_prediction, zero_division=0)\n",
    "\n",
    "    def recall(self) -> float:\n",
    "        \"\"\"Compute Recall = TP / (TP + FN).\"\"\"\n",
    "        return recall_score(self.ground_truth, self.model_prediction, zero_division=0)\n",
    "\n",
    "    def f1_score(self) -> float:\n",
    "        \"\"\"Compute F1 Score = 2 * (Precision * Recall) / (Precision + Recall).\"\"\"\n",
    "        return f1_score(self.ground_truth, self.model_prediction, zero_division=0)\n",
    "\n",
    "    def accuracy(self) -> float:\n",
    "        \"\"\"Compute Accuracy = (TP + TN) / Total Samples.\"\"\"\n",
    "        return accuracy_score(self.ground_truth, self.model_prediction)\n",
    "\n",
    "    def confusion_matrix(self) -> np.ndarray:\n",
    "        return confusion_matrix(self.ground_truth, self.model_prediction)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset stored predictions and ground truth labels.\"\"\"\n",
    "        self.ground_truth = []\n",
    "        self.model_prediction = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWIAXIjKfWQj"
   },
   "source": [
    "**`Define the network `**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1742153937927,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "moHq_TJpWwn4"
   },
   "outputs": [],
   "source": [
    "class LightweightResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(LightweightResidualBlock, self).__init__()\n",
    "\n",
    "        # Depthwise separable convolution for efficiency\n",
    "        self.depthwise = nn.Conv2d(input_channels, input_channels, kernel_size=3, stride=2,\n",
    "                                   padding=1, groups=input_channels)\n",
    "        self.pointwise = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "        # Second depthwise separable convolution\n",
    "        self.depthwise2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, stride=1,\n",
    "                                    padding=1, groups=output_channels)\n",
    "        self.pointwise2 = nn.Conv2d(output_channels, output_channels, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "        # Skip connection with projection\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.depthwise2(out)\n",
    "        out = self.pointwise2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class LightweightFallDetectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LightweightFallDetectionCNN, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.res_block1 = LightweightResidualBlock(32, 64)\n",
    "        self.res_block2 = LightweightResidualBlock(64, 96)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.classifier = nn.Linear(in_features=96, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1742154001117,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "7fcjldTwUHR4"
   },
   "outputs": [],
   "source": [
    "model = LightweightFallDetectionCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1742154001787,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "7fp9L2IvUQSt",
    "outputId": "0f0192df-0e1e-4bf1-ab49-1e5a90e115d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LightweightFallDetectionCNN              --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       2,432\n",
       "│    └─BatchNorm2d: 2-2                  64\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─MaxPool2d: 2-4                    --\n",
       "├─LightweightResidualBlock: 1-2          --\n",
       "│    └─Conv2d: 2-5                       320\n",
       "│    └─Conv2d: 2-6                       2,112\n",
       "│    └─BatchNorm2d: 2-7                  128\n",
       "│    └─Conv2d: 2-8                       640\n",
       "│    └─Conv2d: 2-9                       4,160\n",
       "│    └─BatchNorm2d: 2-10                 128\n",
       "│    └─Sequential: 2-11                  --\n",
       "│    │    └─Conv2d: 3-1                  2,112\n",
       "│    │    └─BatchNorm2d: 3-2             128\n",
       "│    └─ReLU: 2-12                        --\n",
       "├─LightweightResidualBlock: 1-3          --\n",
       "│    └─Conv2d: 2-13                      640\n",
       "│    └─Conv2d: 2-14                      6,240\n",
       "│    └─BatchNorm2d: 2-15                 192\n",
       "│    └─Conv2d: 2-16                      960\n",
       "│    └─Conv2d: 2-17                      9,312\n",
       "│    └─BatchNorm2d: 2-18                 192\n",
       "│    └─Sequential: 2-19                  --\n",
       "│    │    └─Conv2d: 3-3                  6,240\n",
       "│    │    └─BatchNorm2d: 3-4             192\n",
       "│    └─ReLU: 2-20                        --\n",
       "├─AdaptiveAvgPool2d: 1-4                 --\n",
       "├─Linear: 1-5                            97\n",
       "=================================================================\n",
       "Total params: 36,289\n",
       "Trainable params: 36,289\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1741793534549,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "k_ouLTcjYxd-",
    "outputId": "4d66ed67-dc4d-4d84-ba91-4b9863030153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightweightFallDetectionCNN(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res_block1): LightweightResidualBlock(\n",
      "    (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
      "    (pointwise): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "    (pointwise2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (skip): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (res_block2): LightweightResidualBlock(\n",
      "    (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
      "    (pointwise): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (depthwise2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "    (pointwise2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (skip): Sequential(\n",
      "      (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Linear(in_features=96, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48326,
     "status": "ok",
     "timestamp": 1741783817031,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "c5BDgS1uZcCf",
    "outputId": "1ca0fc13-ffa1-4051-a78a-f3e13de26ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfit Epoch 1/100 - Loss: 0.695149\n",
      "Overfit Epoch 2/100 - Loss: 0.685495\n",
      "Overfit Epoch 3/100 - Loss: 0.677610\n",
      "Overfit Epoch 4/100 - Loss: 0.670138\n",
      "Overfit Epoch 5/100 - Loss: 0.662865\n",
      "Overfit Epoch 6/100 - Loss: 0.655639\n",
      "Overfit Epoch 7/100 - Loss: 0.648173\n",
      "Overfit Epoch 8/100 - Loss: 0.640368\n",
      "Overfit Epoch 9/100 - Loss: 0.632074\n",
      "Overfit Epoch 10/100 - Loss: 0.623285\n",
      "Overfit Epoch 11/100 - Loss: 0.614008\n",
      "Overfit Epoch 12/100 - Loss: 0.604349\n",
      "Overfit Epoch 13/100 - Loss: 0.594499\n",
      "Overfit Epoch 14/100 - Loss: 0.584430\n",
      "Overfit Epoch 15/100 - Loss: 0.574207\n",
      "Overfit Epoch 16/100 - Loss: 0.564024\n",
      "Overfit Epoch 17/100 - Loss: 0.554086\n",
      "Overfit Epoch 18/100 - Loss: 0.544398\n",
      "Overfit Epoch 19/100 - Loss: 0.534870\n",
      "Overfit Epoch 20/100 - Loss: 0.525671\n",
      "Overfit Epoch 21/100 - Loss: 0.516572\n",
      "Overfit Epoch 22/100 - Loss: 0.507643\n",
      "Overfit Epoch 23/100 - Loss: 0.498549\n",
      "Overfit Epoch 24/100 - Loss: 0.489009\n",
      "Overfit Epoch 25/100 - Loss: 0.479371\n",
      "Overfit Epoch 26/100 - Loss: 0.468794\n",
      "Overfit Epoch 27/100 - Loss: 0.457541\n",
      "Overfit Epoch 28/100 - Loss: 0.445492\n",
      "Overfit Epoch 29/100 - Loss: 0.432161\n",
      "Overfit Epoch 30/100 - Loss: 0.417956\n",
      "Overfit Epoch 31/100 - Loss: 0.403669\n",
      "Overfit Epoch 32/100 - Loss: 0.389872\n",
      "Overfit Epoch 33/100 - Loss: 0.373034\n",
      "Overfit Epoch 34/100 - Loss: 0.355564\n",
      "Overfit Epoch 35/100 - Loss: 0.339207\n",
      "Overfit Epoch 36/100 - Loss: 0.322926\n",
      "Overfit Epoch 37/100 - Loss: 0.305370\n",
      "Overfit Epoch 38/100 - Loss: 0.286315\n",
      "Overfit Epoch 39/100 - Loss: 0.268619\n",
      "Overfit Epoch 40/100 - Loss: 0.250613\n",
      "Overfit Epoch 41/100 - Loss: 0.233768\n",
      "Overfit Epoch 42/100 - Loss: 0.216935\n",
      "Overfit Epoch 43/100 - Loss: 0.200356\n",
      "Overfit Epoch 44/100 - Loss: 0.184772\n",
      "Overfit Epoch 45/100 - Loss: 0.170185\n",
      "Overfit Epoch 46/100 - Loss: 0.156208\n",
      "Overfit Epoch 47/100 - Loss: 0.142667\n",
      "Overfit Epoch 48/100 - Loss: 0.130344\n",
      "Overfit Epoch 49/100 - Loss: 0.118942\n",
      "Overfit Epoch 50/100 - Loss: 0.109083\n",
      "Overfit Epoch 51/100 - Loss: 0.099854\n",
      "Overfit Epoch 52/100 - Loss: 0.090763\n",
      "Overfit Epoch 53/100 - Loss: 0.083089\n",
      "Overfit Epoch 54/100 - Loss: 0.076065\n",
      "Overfit Epoch 55/100 - Loss: 0.069416\n",
      "Overfit Epoch 56/100 - Loss: 0.063553\n",
      "Overfit Epoch 57/100 - Loss: 0.058278\n",
      "Overfit Epoch 58/100 - Loss: 0.053337\n",
      "Overfit Epoch 59/100 - Loss: 0.049155\n",
      "Overfit Epoch 60/100 - Loss: 0.045152\n",
      "Overfit Epoch 61/100 - Loss: 0.041555\n",
      "Overfit Epoch 62/100 - Loss: 0.038469\n",
      "Overfit Epoch 63/100 - Loss: 0.035440\n",
      "Overfit Epoch 64/100 - Loss: 0.032871\n",
      "Overfit Epoch 65/100 - Loss: 0.030532\n",
      "Overfit Epoch 66/100 - Loss: 0.028382\n",
      "Overfit Epoch 67/100 - Loss: 0.026508\n",
      "Overfit Epoch 68/100 - Loss: 0.024754\n",
      "Overfit Epoch 69/100 - Loss: 0.023165\n",
      "Overfit Epoch 70/100 - Loss: 0.021736\n",
      "Overfit Epoch 71/100 - Loss: 0.020411\n",
      "Overfit Epoch 72/100 - Loss: 0.019253\n",
      "Overfit Epoch 73/100 - Loss: 0.018151\n",
      "Overfit Epoch 74/100 - Loss: 0.017190\n",
      "Overfit Epoch 75/100 - Loss: 0.016282\n",
      "Overfit Epoch 76/100 - Loss: 0.015466\n",
      "Overfit Epoch 77/100 - Loss: 0.014702\n",
      "Overfit Epoch 78/100 - Loss: 0.014001\n",
      "Overfit Epoch 79/100 - Loss: 0.013375\n",
      "Overfit Epoch 80/100 - Loss: 0.012763\n",
      "Overfit Epoch 81/100 - Loss: 0.012223\n",
      "Overfit Epoch 82/100 - Loss: 0.011710\n",
      "Overfit Epoch 83/100 - Loss: 0.011244\n",
      "Overfit Epoch 84/100 - Loss: 0.010800\n",
      "Overfit Epoch 85/100 - Loss: 0.010400\n",
      "Overfit Epoch 86/100 - Loss: 0.010011\n",
      "Overfit Epoch 87/100 - Loss: 0.009663\n",
      "Overfit Epoch 88/100 - Loss: 0.009328\n",
      "Overfit Epoch 89/100 - Loss: 0.009015\n",
      "Overfit Epoch 90/100 - Loss: 0.008724\n",
      "Overfit Epoch 91/100 - Loss: 0.008452\n",
      "Overfit Epoch 92/100 - Loss: 0.008190\n",
      "Overfit Epoch 93/100 - Loss: 0.007949\n",
      "Overfit Epoch 94/100 - Loss: 0.007718\n",
      "Overfit Epoch 95/100 - Loss: 0.007503\n",
      "Overfit Epoch 96/100 - Loss: 0.007295\n",
      "Overfit Epoch 97/100 - Loss: 0.007104\n",
      "Overfit Epoch 98/100 - Loss: 0.006919\n",
      "Overfit Epoch 99/100 - Loss: 0.006742\n",
      "Overfit Epoch 100/100 - Loss: 0.006576\n"
     ]
    }
   ],
   "source": [
    "# To check if the network is learning first we try to overfit over a small batch\n",
    "\n",
    "def overfit_single_batch(model, dataloader, criterion, optimizer, scheduler, device, epochs=100, patience=10):\n",
    "    # Retrieve the batch at index 1 (the second batch)\n",
    "    single_batch = None\n",
    "    for index, (images, labels) in enumerate(dataloader):\n",
    "        if index == 1:\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            single_batch = (images.to(device), labels.to(device))\n",
    "            break\n",
    "    if single_batch is None:\n",
    "        raise RuntimeError(\"Batch with index 1 not found in the dataloader.\")\n",
    "\n",
    "    batch_images, batch_labels = single_batch\n",
    "\n",
    "    # Initialize early stopping variables\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Overfit on this single batch for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Overfit Epoch {epoch+1}/{epochs} - Loss: {loss.item():.6f}\")\n",
    "        scheduler.step(loss)  # Adjust learning rate based on loss\n",
    "\n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0  # Reset counter if loss improves\n",
    "        else:\n",
    "            patience_counter += 1  # Increase counter if loss doesn't improve\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs. Best loss: {best_loss:.6f}\")\n",
    "            break  # Stop training\n",
    "\n",
    "def main():\n",
    "    model = LightweightFallDetectionCNN()\n",
    "    model.train()\n",
    "\n",
    "    dataset = BinaryFallDataset(root_directory_path=DATASET_PATH, subset=\"train\")\n",
    "    dataset.load_dataset()\n",
    "    data_loader = dataset.get_data_loader(batch_size=16)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    overfit_single_batch(model, data_loader, criterion, optimizer, scheduler, device, epochs=100, patience=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Zwy8n0-ed70"
   },
   "source": [
    "**Let's define the training pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1742154109373,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "gEn6IIggjBA1"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, training_loader, validation_loader, loss_criterion, computing_device, metrics_calculator):\n",
    "        \"\"\"Initialize the trainer with model, data, and settings.\"\"\"\n",
    "        self.model = model.to(computing_device)\n",
    "        self.training_loader = training_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=SCHEDULER_STEP, gamma=SCHEDULER_GAMMA)\n",
    "        self.computing_device = computing_device\n",
    "        self.number_of_epochs = NUMBER_OF_EPOCHS\n",
    "        self.early_stopping_patience = EARLY_STOP_PATIENCE\n",
    "        self.tensorboard_writer = SummaryWriter()\n",
    "        self.metrics_calculator = metrics_calculator\n",
    "\n",
    "        # Create directory in Google Drive to save model\n",
    "        self.experiment_path = self.create_experiment_folder()\n",
    "\n",
    "    def create_experiment_folder(self):\n",
    "        \"\"\"Creates an experiment folder in Google Drive with timestamp.\"\"\"\n",
    "\n",
    "        # Define base path and timestamp\n",
    "        base_path = \"/content/drive/MyDrive/experiment\"\n",
    "        timestamp = datetime.now().strftime(\"exp_%d_%m_%Y_%H_%M\")\n",
    "        experiment_path = os.path.join(base_path, timestamp)\n",
    "\n",
    "        # Create folder if it doesn't exist\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        print(f\"Experiment directory created at: {experiment_path}\")\n",
    "\n",
    "        return experiment_path\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"Train the model for one epoch and return only the loss.\"\"\"\n",
    "        self.model.train()\n",
    "        total_training_loss = 0.0\n",
    "\n",
    "        for images, labels in self.training_loader:\n",
    "            images = images.to(self.computing_device)\n",
    "            labels = labels.to(self.computing_device).float().unsqueeze(1)  # For BCEWithLogitsLoss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.loss_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_training_loss += loss.item() * batch_size\n",
    "\n",
    "        average_training_loss = total_training_loss / len(self.training_loader.dataset)\n",
    "        return average_training_loss\n",
    "\n",
    "    def validate_one_epoch(self):\n",
    "        \"\"\"Validate the model and return loss plus all metrics.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.metrics_calculator.reset()\n",
    "        total_validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.validation_loader:\n",
    "                images = images.to(self.computing_device)\n",
    "                labels = labels.to(self.computing_device).float().unsqueeze(1)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_criterion(outputs, labels)\n",
    "\n",
    "                total_validation_loss += loss.item() * images.size(0)\n",
    "                self.metrics_calculator.convert_logits(outputs.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "\n",
    "        average_validation_loss = total_validation_loss / len(self.validation_loader.dataset)\n",
    "        validation_accuracy = self.metrics_calculator.accuracy()\n",
    "        validation_precision = self.metrics_calculator.precision()\n",
    "        validation_recall = self.metrics_calculator.recall()\n",
    "        validation_f1_score = self.metrics_calculator.f1_score()\n",
    "        return average_validation_loss, validation_accuracy, validation_precision, validation_recall, validation_f1_score\n",
    "\n",
    "    def train_full_cycle(self):\n",
    "        \"\"\"Run the full training loop with early stopping.\"\"\"\n",
    "        best_validation_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        print(\"device: \", computing_device)\n",
    "\n",
    "        for epoch in range(self.number_of_epochs):\n",
    "            print(f\"\\n=== Epoch {epoch + 1}/{self.number_of_epochs} ===\")\n",
    "\n",
    "            # Train (only loss) and validate (loss + metrics)\n",
    "            training_loss = self.train_one_epoch()\n",
    "            validation_loss, validation_accuracy, validation_precision, validation_recall, validation_f1_score = self.validate_one_epoch()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            self.tensorboard_writer.add_scalars(\"Loss\", {\"Training\": training_loss, \"Validation\": validation_loss}, epoch)\n",
    "            self.tensorboard_writer.add_scalar(\"Validation/Accuracy\", validation_accuracy, epoch)\n",
    "            self.tensorboard_writer.add_scalar(\"Validation/Precision\", validation_precision, epoch)\n",
    "            self.tensorboard_writer.add_scalar(\"Validation/Recall\", validation_recall, epoch)\n",
    "            self.tensorboard_writer.add_scalar(\"Validation/F1 Score\", validation_f1_score, epoch)\n",
    "\n",
    "            # Print metrics (training loss only, full validation metrics)\n",
    "            print(f\"Training   - Loss: {training_loss:.4f}\")\n",
    "            print(f\"Validation - Loss: {validation_loss:.4f} | Accuracy: {validation_accuracy:.4f} | Precision: {validation_precision:.4f} | Recall: {validation_recall:.4f} | F1: {validation_f1_score:.4f}\")\n",
    "\n",
    "            # Early stopping and model saving based on validation loss\n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                epochs_without_improvement = 0\n",
    "                model_save_path = os.path.join(self.experiment_path, \"best_model.pth\")\n",
    "                torch.save(self.model.state_dict(), model_save_path)\n",
    "                print(f\"=> Saved best model at: {model_save_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= self.early_stopping_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        self.tensorboard_writer.close()\n",
    "        print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1mHBGwXiXYr"
   },
   "source": [
    "**Define training hyper parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxZKK2TgicFa"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUMBER_OF_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Early stopping settings\n",
    "EARLY_STOP_PATIENCE = 8\n",
    "\n",
    "# Learning rate scheduler settings\n",
    "SCHEDULER_STEP = 4\n",
    "SCHEDULER_GAMMA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6812379,
     "status": "ok",
     "timestamp": 1741792778869,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "DGz9EaISmLX4",
    "outputId": "16510373-2ba2-4d18-b9f4-cd0a84e73f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory created at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26\n",
      "device:  cuda\n",
      "\n",
      "=== Epoch 1/100 ===\n",
      "Training   - Loss: 0.3396\n",
      "Validation - Loss: 0.2359 | Accuracy: 0.9123 | Precision: 0.9057 | Recall: 0.9302 | F1: 0.9178\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 2/100 ===\n",
      "Training   - Loss: 0.1527\n",
      "Validation - Loss: 0.1186 | Accuracy: 0.9519 | Precision: 0.9810 | Recall: 0.9267 | F1: 0.9531\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 3/100 ===\n",
      "Training   - Loss: 0.1073\n",
      "Validation - Loss: 0.0830 | Accuracy: 0.9712 | Precision: 0.9837 | Recall: 0.9613 | F1: 0.9724\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 4/100 ===\n",
      "Training   - Loss: 0.0825\n",
      "Validation - Loss: 0.0757 | Accuracy: 0.9771 | Precision: 0.9792 | Recall: 0.9772 | F1: 0.9782\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 5/100 ===\n",
      "Training   - Loss: 0.0532\n",
      "Validation - Loss: 0.0478 | Accuracy: 0.9800 | Precision: 0.9827 | Recall: 0.9793 | F1: 0.9809\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 6/100 ===\n",
      "Training   - Loss: 0.0473\n",
      "Validation - Loss: 0.0472 | Accuracy: 0.9814 | Precision: 0.9922 | Recall: 0.9723 | F1: 0.9822\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 7/100 ===\n",
      "Training   - Loss: 0.0427\n",
      "Validation - Loss: 0.0520 | Accuracy: 0.9803 | Precision: 0.9943 | Recall: 0.9682 | F1: 0.9811\n",
      "\n",
      "=== Epoch 8/100 ===\n",
      "Training   - Loss: 0.0354\n",
      "Validation - Loss: 0.0376 | Accuracy: 0.9873 | Precision: 0.9903 | Recall: 0.9855 | F1: 0.9879\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 9/100 ===\n",
      "Training   - Loss: 0.0247\n",
      "Validation - Loss: 0.0322 | Accuracy: 0.9916 | Precision: 0.9924 | Recall: 0.9917 | F1: 0.9920\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 10/100 ===\n",
      "Training   - Loss: 0.0221\n",
      "Validation - Loss: 0.0388 | Accuracy: 0.9905 | Precision: 0.9890 | Recall: 0.9931 | F1: 0.9910\n",
      "\n",
      "=== Epoch 11/100 ===\n",
      "Training   - Loss: 0.0189\n",
      "Validation - Loss: 0.0293 | Accuracy: 0.9905 | Precision: 0.9931 | Recall: 0.9889 | F1: 0.9910\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 12/100 ===\n",
      "Training   - Loss: 0.0192\n",
      "Validation - Loss: 0.0271 | Accuracy: 0.9931 | Precision: 0.9938 | Recall: 0.9931 | F1: 0.9934\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 13/100 ===\n",
      "Training   - Loss: 0.0144\n",
      "Validation - Loss: 0.0258 | Accuracy: 0.9934 | Precision: 0.9924 | Recall: 0.9952 | F1: 0.9938\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 14/100 ===\n",
      "Training   - Loss: 0.0125\n",
      "Validation - Loss: 0.0298 | Accuracy: 0.9916 | Precision: 0.9938 | Recall: 0.9903 | F1: 0.9920\n",
      "\n",
      "=== Epoch 15/100 ===\n",
      "Training   - Loss: 0.0134\n",
      "Validation - Loss: 0.0250 | Accuracy: 0.9931 | Precision: 0.9904 | Recall: 0.9965 | F1: 0.9935\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 16/100 ===\n",
      "Training   - Loss: 0.0103\n",
      "Validation - Loss: 0.0272 | Accuracy: 0.9927 | Precision: 0.9938 | Recall: 0.9924 | F1: 0.9931\n",
      "\n",
      "=== Epoch 17/100 ===\n",
      "Training   - Loss: 0.0084\n",
      "Validation - Loss: 0.0224 | Accuracy: 0.9931 | Precision: 0.9938 | Recall: 0.9931 | F1: 0.9934\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 18/100 ===\n",
      "Training   - Loss: 0.0083\n",
      "Validation - Loss: 0.0226 | Accuracy: 0.9934 | Precision: 0.9938 | Recall: 0.9938 | F1: 0.9938\n",
      "\n",
      "=== Epoch 19/100 ===\n",
      "Training   - Loss: 0.0081\n",
      "Validation - Loss: 0.0209 | Accuracy: 0.9934 | Precision: 0.9931 | Recall: 0.9945 | F1: 0.9938\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 20/100 ===\n",
      "Training   - Loss: 0.0073\n",
      "Validation - Loss: 0.0248 | Accuracy: 0.9927 | Precision: 0.9945 | Recall: 0.9917 | F1: 0.9931\n",
      "\n",
      "=== Epoch 21/100 ===\n",
      "Training   - Loss: 0.0072\n",
      "Validation - Loss: 0.0231 | Accuracy: 0.9938 | Precision: 0.9951 | Recall: 0.9931 | F1: 0.9941\n",
      "\n",
      "=== Epoch 22/100 ===\n",
      "Training   - Loss: 0.0070\n",
      "Validation - Loss: 0.0202 | Accuracy: 0.9945 | Precision: 0.9945 | Recall: 0.9952 | F1: 0.9948\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 23/100 ===\n",
      "Training   - Loss: 0.0069\n",
      "Validation - Loss: 0.0223 | Accuracy: 0.9938 | Precision: 0.9938 | Recall: 0.9945 | F1: 0.9941\n",
      "\n",
      "=== Epoch 24/100 ===\n",
      "Training   - Loss: 0.0055\n",
      "Validation - Loss: 0.0228 | Accuracy: 0.9942 | Precision: 0.9952 | Recall: 0.9938 | F1: 0.9945\n",
      "\n",
      "=== Epoch 25/100 ===\n",
      "Training   - Loss: 0.0061\n",
      "Validation - Loss: 0.0205 | Accuracy: 0.9945 | Precision: 0.9952 | Recall: 0.9945 | F1: 0.9948\n",
      "\n",
      "=== Epoch 26/100 ===\n",
      "Training   - Loss: 0.0058\n",
      "Validation - Loss: 0.0214 | Accuracy: 0.9949 | Precision: 0.9958 | Recall: 0.9945 | F1: 0.9952\n",
      "\n",
      "=== Epoch 27/100 ===\n",
      "Training   - Loss: 0.0057\n",
      "Validation - Loss: 0.0210 | Accuracy: 0.9945 | Precision: 0.9945 | Recall: 0.9952 | F1: 0.9948\n",
      "\n",
      "=== Epoch 28/100 ===\n",
      "Training   - Loss: 0.0055\n",
      "Validation - Loss: 0.0221 | Accuracy: 0.9953 | Precision: 0.9958 | Recall: 0.9952 | F1: 0.9955\n",
      "\n",
      "=== Epoch 29/100 ===\n",
      "Training   - Loss: 0.0057\n",
      "Validation - Loss: 0.0206 | Accuracy: 0.9942 | Precision: 0.9952 | Recall: 0.9938 | F1: 0.9945\n",
      "\n",
      "=== Epoch 30/100 ===\n",
      "Training   - Loss: 0.0051\n",
      "Validation - Loss: 0.0201 | Accuracy: 0.9949 | Precision: 0.9945 | Recall: 0.9959 | F1: 0.9952\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 31/100 ===\n",
      "Training   - Loss: 0.0050\n",
      "Validation - Loss: 0.0214 | Accuracy: 0.9942 | Precision: 0.9952 | Recall: 0.9938 | F1: 0.9945\n",
      "\n",
      "=== Epoch 32/100 ===\n",
      "Training   - Loss: 0.0053\n",
      "Validation - Loss: 0.0195 | Accuracy: 0.9949 | Precision: 0.9958 | Recall: 0.9945 | F1: 0.9952\n",
      "=> Saved best model at: /content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\n",
      "\n",
      "=== Epoch 33/100 ===\n",
      "Training   - Loss: 0.0054\n",
      "Validation - Loss: 0.0199 | Accuracy: 0.9945 | Precision: 0.9965 | Recall: 0.9931 | F1: 0.9948\n",
      "\n",
      "=== Epoch 34/100 ===\n",
      "Training   - Loss: 0.0054\n",
      "Validation - Loss: 0.0200 | Accuracy: 0.9953 | Precision: 0.9952 | Recall: 0.9959 | F1: 0.9955\n",
      "\n",
      "=== Epoch 35/100 ===\n",
      "Training   - Loss: 0.0050\n",
      "Validation - Loss: 0.0205 | Accuracy: 0.9942 | Precision: 0.9952 | Recall: 0.9938 | F1: 0.9945\n",
      "\n",
      "=== Epoch 36/100 ===\n",
      "Training   - Loss: 0.0049\n",
      "Validation - Loss: 0.0222 | Accuracy: 0.9938 | Precision: 0.9965 | Recall: 0.9917 | F1: 0.9941\n",
      "\n",
      "=== Epoch 37/100 ===\n",
      "Training   - Loss: 0.0051\n",
      "Validation - Loss: 0.0205 | Accuracy: 0.9938 | Precision: 0.9938 | Recall: 0.9945 | F1: 0.9941\n",
      "\n",
      "=== Epoch 38/100 ===\n",
      "Training   - Loss: 0.0055\n",
      "Validation - Loss: 0.0202 | Accuracy: 0.9949 | Precision: 0.9958 | Recall: 0.9945 | F1: 0.9952\n",
      "\n",
      "=== Epoch 39/100 ===\n",
      "Training   - Loss: 0.0048\n",
      "Validation - Loss: 0.0210 | Accuracy: 0.9953 | Precision: 0.9931 | Recall: 0.9979 | F1: 0.9955\n",
      "\n",
      "=== Epoch 40/100 ===\n",
      "Training   - Loss: 0.0048\n",
      "Validation - Loss: 0.0220 | Accuracy: 0.9942 | Precision: 0.9958 | Recall: 0.9931 | F1: 0.9945\n",
      "Early stopping triggered.\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual dataset path\n",
    "    computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize datasets\n",
    "    training_dataset = BinaryFallDataset(DATASET_PATH, subset=\"train\")\n",
    "    validation_dataset = BinaryFallDataset(DATASET_PATH, subset=\"validation\")\n",
    "    training_dataset.load_dataset()\n",
    "    validation_dataset.load_dataset()\n",
    "\n",
    "    # Get data loaders\n",
    "    training_loader = training_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n",
    "    validation_loader = validation_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n",
    "\n",
    "    # Initialize model, criterion, and metrics\n",
    "    model = LightweightFallDetectionCNN()\n",
    "    loss_criterion = nn.BCEWithLogitsLoss()\n",
    "    metrics_calculator = MetricsCalculator()\n",
    "\n",
    "    # Initialize and run trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        training_loader=training_loader,\n",
    "        validation_loader=validation_loader,\n",
    "        loss_criterion=loss_criterion,\n",
    "        computing_device=computing_device,\n",
    "        metrics_calculator=metrics_calculator\n",
    "    )\n",
    "    trainer.train_full_cycle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp9ke62DerUk"
   },
   "source": [
    "**Evaluation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z24xrWR2f52i"
   },
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, model, evaluation_loader, loss_criterion, computing_device, metrics_calculator):\n",
    "        \"\"\"Initialize the evaluator with model, data, and settings.\"\"\"\n",
    "        self.model = model.to(computing_device)\n",
    "        self.evaluation_loader = evaluation_loader\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.computing_device = computing_device\n",
    "        self.metrics_calculator = metrics_calculator\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model on the evaluation dataset and return loss, metrics, and confusion matrix.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.metrics_calculator.reset()\n",
    "        total_evaluation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.evaluation_loader:\n",
    "                images = images.to(self.computing_device)\n",
    "                labels = labels.to(self.computing_device).float().unsqueeze(1)  # For BCEWithLogitsLoss\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_criterion(outputs, labels)\n",
    "\n",
    "                total_evaluation_loss += loss.item() * images.size(0)\n",
    "                self.metrics_calculator.convert_logits(outputs.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "\n",
    "        # Compute average loss and metrics\n",
    "        average_evaluation_loss = total_evaluation_loss / len(self.evaluation_loader.dataset)\n",
    "        evaluation_accuracy = self.metrics_calculator.accuracy()\n",
    "        evaluation_precision = self.metrics_calculator.precision()\n",
    "        evaluation_recall = self.metrics_calculator.recall()\n",
    "        evaluation_f1_score = self.metrics_calculator.f1_score()\n",
    "        evaluation_confusion_matrix = self.metrics_calculator.confusion_matrix()\n",
    "\n",
    "        return (\n",
    "            average_evaluation_loss,\n",
    "            evaluation_accuracy,\n",
    "            evaluation_precision,\n",
    "            evaluation_recall,\n",
    "            evaluation_f1_score,\n",
    "            evaluation_confusion_matrix\n",
    "        )\n",
    "\n",
    "    def print_evaluation_results(self):\n",
    "        \"\"\"Print the evaluation results in a formatted way.\"\"\"\n",
    "        (\n",
    "            loss,\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1_score,\n",
    "            confusion_matrix\n",
    "        ) = self.evaluate()\n",
    "\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"[[TN, FP]\")\n",
    "        print(\" [FN, TP]]\")\n",
    "        print(f\"{confusion_matrix[0][0]:4d}  {confusion_matrix[0][1]:4d}\")\n",
    "        print(f\"{confusion_matrix[1][0]:4d}  {confusion_matrix[1][1]:4d}\")\n",
    "        print(\"\\nWhere:\")\n",
    "        print(f\"TN (True Negatives)  = {confusion_matrix[0][0]:4d} : Correctly predicted negatives\")\n",
    "        print(f\"FP (False Positives) = {confusion_matrix[0][1]:4d} : Negatives predicted as positives\")\n",
    "        print(f\"FN (False Negatives) = {confusion_matrix[1][0]:4d} : Positives predicted as negatives\")\n",
    "        print(f\"TP (True Positives)  = {confusion_matrix[1][1]:4d} : Correctly predicted positives\")\n",
    "        print(\"======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 719591,
     "status": "ok",
     "timestamp": 1741794314488,
     "user": {
      "displayName": "Fear 13",
      "userId": "14772979768791465479"
     },
     "user_tz": -60
    },
    "id": "HH5vam_pjMCt",
    "outputId": "84e9d080-f1c3-4fe4-cbf6-3b38f1b2d2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Loss: 0.0167\n",
      "Accuracy: 0.9949\n",
      "Precision: 0.9965\n",
      "Recall: 0.9938\n",
      "F1 Score: 0.9952\n",
      "\n",
      "Confusion Matrix:\n",
      "[[TN, FP]\n",
      " [FN, TP]]\n",
      "1297     5\n",
      "   9  1438\n",
      "\n",
      "Where:\n",
      "TN (True Negatives)  = 1297 : Correctly predicted negatives\n",
      "FP (False Positives) =    5 : Negatives predicted as positives\n",
      "FN (False Negatives) =    9 : Positives predicted as negatives\n",
      "TP (True Positives)  = 1438 : Correctly predicted positives\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model = LightweightFallDetectionCNN()\n",
    "computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loss_criterion = nn.BCEWithLogitsLoss()\n",
    "metrics_calculator = MetricsCalculator()\n",
    "\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/experiment/exp_12_03_2025_13_26/best_model.pth\", weights_only=True))\n",
    "model.to(computing_device)\n",
    "\n",
    "# Test dataset and DataLoader (from BinaryFallDataset)\n",
    "test_dataset = BinaryFallDataset(root_directory_path=Path(DATASET_PATH), subset=\"test\")\n",
    "test_dataset.load_dataset()\n",
    "test_loader = test_dataset.get_data_loader(batch_size=32, shuffle_data=False)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = Evaluation(\n",
    "    model=model,\n",
    "    evaluation_loader=test_loader,\n",
    "    loss_criterion=loss_criterion,\n",
    "    computing_device=computing_device,\n",
    "    metrics_calculator=metrics_calculator\n",
    ")\n",
    "\n",
    "# Run evaluation and print results\n",
    "evaluator.print_evaluation_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEdypMK7a0JTZzwmKj4KLL",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1fh26Sroabn0IawoM1rq-9afMLwPXWz_8",
   "provenance": [
    {
     "file_id": "1x-lLXBo89L_KwXERvaouwSwmx27Auk1l",
     "timestamp": 1741706077201
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
